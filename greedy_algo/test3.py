import matplotlib.pyplot as plt
from nll import * 

if __name__ == '__main__':

    train_len = 200
    avgnum = 1
    train_data = None
    with open('data_exp16_train.npy', 'rb') as f:
        train_data = list(np.load(f))
    ############ Training error ##############################33

    # generate true data for a hawkes synthetic process omega=1; mu=0.2; alpha=0.8; history_start=0;
    omega = 1
    np.random.seed(0)
    def predict(mu, alpha, history: History):
        last_time = history.time_slots[-1] + 1e-10
        lambda_max = mu + alpha * \
            np.exp((last_time - history.time_slots) * -1 * omega).sum()
        while (True):
            u = np.random.uniform(0, 1)
            last_time = last_time - (np.log(1-u) / lambda_max)
            u2 = np.random.uniform(0, 1)
            value = (mu + alpha * np.exp((last_time - history.time_slots)
                     * -1 * omega).sum()) / lambda_max
            if u2 <= value:
                break
        return last_time


    train_data = [0]
    mu = 0.2
    alpha = 0.8
    prob_log = 0
    for i in range(1, train_len):
        val = predict(0.2, 0.8, History(train_data[:i]))
        prob_log += np.log(mu + alpha * np.exp((val - History(train_data[:i]).time_slots) * -1 * omega).sum()) - (mu * val) + (alpha/omega * (np.exp(-1 * omega * (val - train_data[-1])) - 1) )
        train_data.append(val)
    print('logprob of actual data', prob_log/train_len)


    # try to get the likelihood of the actual data from another hawkes process of same parameters
    avg = 0
    for j in range(avgnum):
        curr = [0]
        mu = 0.2
        alpha = 0.8
        prob_log = 0
        for i in range(1, train_len):
            val = predict(0.2, 0.8, History(curr[:i]))
            prob_log += np.log(mu + alpha * np.exp((val - History(curr[:i]).time_slots) * -1 * omega).sum()) - mu * val + alpha/omega * (np.exp(-1 * omega * (val - curr[-1])) - 1) 
            curr.append(val)
        # print('logprob', prob_log)
        avg += prob_log/train_len
    print('average logprob of a similar hawkes process', avg/(avgnum))


    # try to get likelihood of actual data when alpha and data are obtained through a non-data minimization of train_len params
    final_T = train_data[-1] + 1e-10
    setting1 = Setting1(None, None, None, final_T, omega, init_num_epochs=300, lr=5e-3, epoch_decay=None, budget=None, sensitivity=None)
    mu, alpha, _ = setting1.do_forward(History(train_data[:(train_len-1)]), [train_data[train_len-1]])
    mu = mu.item()
    alpha = alpha.item()
    print('mu', mu, 'alpha', alpha)
    avg = 0
    for j in range(avgnum):
        curr = [0]
        prob_log = 0
        for i in range(1, train_len):
            val = predict(mu, alpha, History(curr[:i]))
            prob_log += np.log(mu + alpha * np.exp((val - History(curr[:i]).time_slots) * -1 * omega).sum()) - mu * val + alpha/omega * (np.exp(-1 * omega * (val - curr[-1])) - 1) 
            curr.append(val)
        # print('logprob', prob_log)
        avg += prob_log/train_len
    print('average logprob of a hawkes process whose parameters are generated by us', avg/avgnum)


    # try to get likelihood of actual data when alpha and data are obtained through a data minimization of train_len params
    final_T = train_data[-1] + 1e-10
    setting1 = Setting1(60, 0.6, 1e-3, final_T, omega, init_num_epochs=300, lr=5e-3, epoch_decay=0.6, budget=1.0, sensitivity=None)
    new_history = setting1.greedy_algo(History([train_data[0]]), np.asarray(train_data[1:]), True)
    print('new history', new_history.__len__())
    mu, alpha, _ = setting1.do_forward(History(new_history.time_slots[:-1]), new_history.time_slots[-1:])
    mu = mu.item()
    alpha = alpha.item()
    print('mu', mu, 'alpha', alpha)
    avg = 0
    for j in range(avgnum):
        curr = [0]
        prob_log = 0
        for i in range(1, train_len):
            val = predict(mu, alpha, History(curr[:i]))
            prob_log += np.log(mu + alpha * np.exp((val - History(curr[:i]).time_slots) * -1 * omega).sum()) - mu * val + alpha/omega * (np.exp(-1 * omega * (val - curr[-1])) - 1) 
            curr.append(val)
        # print('logprob', prob_log)
        avg += prob_log/train_len
    print('average logprob of a hawkes process whose parameters are generated by us (minimized)', avg/avgnum)

