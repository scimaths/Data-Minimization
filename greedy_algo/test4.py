import matplotlib.pyplot as plt
from nll import * 

if __name__ == '__main__':

    train_len = 800
    val_len = 200
    test_len = 500
    lr_data = 1e-2
    num_epochs = 300
    avgnum = 1
    train_data = None
    with open('data_exp16_train.npy', 'rb') as f:
        train_data = list(np.load(f))
    ############ Training error ##############################33

    # generate true data for a hawkes synthetic process omega=1; mu=0.2; alpha=0.8; history_start=0;
    omega = 1
    np.random.seed(0)
    def predict(mu, alpha, history: History):
        last_time = history.time_slots[-1] + 1e-10
        lambda_max = mu + alpha * \
            np.exp((last_time - history.time_slots) * -1 * omega).sum()
        while (True):
            u = np.random.uniform(0, 1)
            last_time = last_time - (np.log(1-u) / lambda_max)
            u2 = np.random.uniform(0, 1)
            value = (mu + alpha * np.exp((last_time - history.time_slots)
                     * -1 * omega).sum()) / lambda_max
            if u2 <= value:
                break
        return last_time

    
    
    def score(histData, mu, alpha, testData):
        # T = None
        T = np.max(testData)
        curr = histData
        prob_log = 0
        for i in range(len(testData)):
            prob_log += np.log(mu + alpha * np.exp((testData[i] - History(curr).time_slots) * -1 * omega).sum()) 
            prob_log += alpha/omega * (np.exp(-1 * omega * (T - testData[i])) - 1)
            curr.append(testData[i])
        if len(histData):
            prob_log -= mu * (np.max(testData) - np.max(histData))
        else:
            prob_log -= mu * T
        return -1 * prob_log/len(testData)


    train_data = [0]
    mu = 0.2
    alpha = 0.8
    for i in range(1, train_len):
        val = predict(0.2, 0.8, History(train_data[:i]))
        train_data.append(val)
    print('logprob of actual data (training)', score([0], mu, alpha, train_data[1:]))

    temp = train_data.copy()
    val_data = []
    for i in range(val_len):
        val = predict(0.2, 0.8, History(temp))
        temp.append(val)
        val_data.append(val)

    test_data = []
    for i in range(test_len):
        val = predict(0.2, 0.8, History(temp))
        temp.append(val)
        test_data.append(val)
    print('logprob of actual data (test)', score(train_data + val_data, mu, alpha, test_data))

    assert len(train_data) == train_len
    assert len(val_data) == val_len
    assert len(test_data) == test_len




    # try to get likelihood of actual data when alpha and data are obtained through a non-data minimization of train_len params
    # final_T = train_data[-1] + 1e-10
    # setting1 = Setting1(None, None, None, final_T, omega, init_num_epochs=num_epochs, lr=lr_data, epoch_decay=None, budget=None, sensitivity=None)
    # mu, alpha, _ = setting1.do_forward(History(train_data[:(train_len-1)]), None, [train_data[train_len-1]])
    # mu = mu.item()
    # alpha = alpha.item()
    # print('mu', mu, 'alpha', alpha)
    # print('average logprob of a hawkes process whose parameters are generated by us (training)', score([0], mu, alpha, train_data[1:]))
    # print('average logprob of a hawkes process whose parameters are generated by us (test)', score(train_data + val_data, mu, alpha, test_data))



    # # try to get likelihood of actual data when alpha and data are obtained through a data minimization of train_len params
    # final_T = val_data[-1] + 1e-10
    # setting1 = Setting1(200, 0.6, 1e-3, final_T, omega, init_num_epochs=num_epochs, lr=lr_data, epoch_decay=1.0, budget=1.0, sensitivity=None)
    
    # # edit 1: We are choosing the initial value in the minimized training data as train_data[0] (it need not be)

    # new_history = setting1.greedy_algo(History([]), History(val_data), np.asarray(train_data), False)
    # print('new history', new_history.__len__())
    # removed_history = np.setdiff1d(train_data, new_history.time_slots)
    # plt.scatter(removed_history, np.ones((len(removed_history))), color='r')
    # plt.scatter(new_history.time_slots, np.ones((new_history.__len__())), color='b')
    # mu, alpha, _ = setting1.do_forward(History(new_history.time_slots[:-1]), None, new_history.time_slots[-1:])
    # mu = mu.item()
    # alpha = alpha.item()
    # print('mu', mu, 'alpha', alpha)
    # print('average logprob of a hawkes process whose parameters are generated by us (minimized) (test)', score(list(np.concatenate((new_history.time_slots, val_data))), mu, alpha, test_data))



    



    # try to get likelihood of actual data when alpha and data are obtained through a data minimization of train_len params with reverse mode
    final_T = val_data[-1] + 1e-10
    setting1 = Setting1(None, 0.6, 1e-3, final_T, omega, init_num_epochs=num_epochs, lr=lr_data, epoch_decay=1.0, budget=1.0, sensitivity=None)
    
    new_history = setting1.greedy_algo_with_reverse(History(train_data), History(val_data), False)
    print('new history', new_history.__len__())

    removed_history = np.setdiff1d(train_data, new_history.time_slots)
    plt.scatter(removed_history, 0.2 + np.ones((len(removed_history))), color='r')
    plt.scatter(new_history.time_slots, 0.2+ np.ones((new_history.__len__())), color='b')
    
    mu, alpha, _ = setting1.do_forward(History(new_history.time_slots[:-1]), None, new_history.time_slots[-1:])
    mu = mu.item()
    alpha = alpha.item()
    
    print('mu', mu, 'alpha', alpha)
    print('average logprob of a hawkes process whose parameters are generated by us (minimized) (reverse mode) (test)', score(list(np.concatenate((new_history.time_slots, val_data))), mu, alpha, test_data))

    # plt.savefig('time_slot_select.png')












    # try to get likelihood of actual data when alpha and data are obtained through a data minimization sensitivity of train_len params
    # final_T = val_data[-1] + 1e-10
    # setting1 = Setting1(60, 0.6, 1e-3, final_T, omega, init_num_epochs=num_epochs, lr=lr_data, epoch_decay=1.0, budget=1.0, sensitivity=0.1)
    # new_history = setting1.greedy_algo_with_tests(History([train_data[0]]), History(val_data), np.asarray(train_data[1:]), False)
    # print('new history', new_history.__len__())
    # removed_history = np.setdiff1d(train_data, new_history.time_slots)
    # plt.scatter(removed_history, 0.4+np.ones((len(removed_history))), color='r')
    # plt.scatter(new_history.time_slots, 0.4+np.ones((new_history.__len__())), color='b')
    # mu, alpha, _ = setting1.do_forward(History(new_history.time_slots[:-1]), None, new_history.time_slots[-1:])
    # mu = mu.item()
    # alpha = alpha.item()
    # print('mu', mu, 'alpha', alpha)
    # print('average logprob of a hawkes process whose parameters are generated by us (minimized) (sensitivity) (test)', score(list(np.concatenate((new_history.time_slots, val_data))), mu, alpha, test_data))

    # plt.savefig('time_slot_select.png')



    # # try to get likelihood of actual data when alpha and data are obtained through a data minimization of train_len params by randomization
    # final_T = train_data[-1] + 1e-10
    # setting1 = Setting1(60, 0.6, 1e-3, final_T, omega, init_num_epochs=num_epochs, lr=lr_data, epoch_decay=0.6, budget=1.0, sensitivity=None)
    # new_history = np.random.choice(train_data, int(0.6 * train_len), replace=False)
    # print('new history', new_history.__len__())
    # removed_history = np.setdiff1d(train_data, new_history)
    # plt.scatter(removed_history, 0.6 + np.ones((len(removed_history))), color='r')
    # plt.scatter(new_history, 0.6 + np.ones((len(new_history))), color='b')
    # mu, alpha, _ = setting1.do_forward(History(new_history[:-1]), None, new_history[-1:])
    # mu = mu.item()
    # alpha = alpha.item()
    # print('mu', mu, 'alpha', alpha)
    # print('average logprob of a hawkes process whose parameters are generated by us (minimized by random) (test)', score(list(np.concatenate((new_history, val_data))), mu, alpha, test_data))
    # plt.savefig('time_slot_select.png')



    # # try to get likelihood of actual data when alpha and data are obtained through a data minimization of train_len params by most recent
    # final_T = train_data[-1] + 1e-10
    # setting1 = Setting1(60, 0.6, 1e-3, final_T, omega, init_num_epochs=num_epochs, lr=lr_data, epoch_decay=0.6, budget=1.0, sensitivity=None)
    # new_history = train_data[int(0.4 * train_len):]
    # print('new history', new_history.__len__())
    # removed_history = np.setdiff1d(train_data, new_history)
    # plt.scatter(removed_history, 0.8 + np.ones((len(removed_history))), color='r')
    # plt.scatter(new_history, 0.8 + np.ones((len(new_history))), color='b')
    # mu, alpha, _ = setting1.do_forward(History(new_history[:-1]), None, new_history[-1:])
    # mu = mu.item()
    # alpha = alpha.item()
    # print('mu', mu, 'alpha', alpha)
    # print('average logprob of a hawkes process whose parameters are generated by us (minimized by recent) (test)', score(list(np.concatenate((new_history, val_data))) , mu, alpha, test_data))

    # plt.savefig('time_slot_select.png')